---
title: "Assignment3"
output:
  pdf_document: default
  html_document: default
date: "2023-10-23"
---
```{r}
library(dada2); packageVersion("dada2")
```
```{r}
#making a object called path
path<- "/Users/anxinzhao/Desktop/assignment3"
list.files(path)
```
```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```


```{r}
#visualize the quality profiles of the forward reads:
plotQualityProfile(fnFs[1:2])
#visualize the quality profile of the reverse reads,(look at 1st and 2nd samples, subsetting 2 samples):
plotQualityProfile(fnRs[1:2])

```
```{r}
# Place filtered sample in filtered/ subdirectory
#making a filtered file for both forward and reverse read
#sample name
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
#filter and trim
#sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores
# estimate 180 and 250 from the plots
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(250,180),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
head(out)
```
```{r}
#learn the error rates
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
```
```{r}
#sample inference
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

#Inspecting the returned dada-class object:
dadaFs[[1]]
```

```{r}
#merge paired read
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

```{r}
#Construct sequence table
#construct an amplicon sequence variant table (ASV) table called seqtab
seqtab <- makeSequenceTable(mergers)
dim(seqtab)


# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

```{r}
#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)
```

```{r}
#Track reads through the pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

```{r}
#Assign taxonomy
taxa <- assignTaxonomy(seqtab.nochim, "~/Desktop/assignment3/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
```


```{r}
#make csv files
write.csv(taxa, file ="/Users/anxinzhao/Desktop/assignment3/taxa.csv")
write.csv(seqtab.nochim, file = "/Users/anxinzhao/Desktop/assignment3/seqtab.nochim.csv")
```

```{r}
#read csv files into R
taxa<- read.csv(file= "/Users/anxinzhao/Desktop/assignment3/taxa.csv", sep=",", row.names=1)
seqtab.nochim<- read.csv(file= "/Users/anxinzhao/Desktop/assignment3/seqtab.nochim.csv", sep=",", row.names=1)
```

```{r}
#filp seqtab.nochim
flipped_seqtab.nochim<-as.data.frame (t(seqtab.nochim))
                                     
dim(seqtab.nochim)
```
```{r}
#combine flipped_seqtab and taxa in to one csv file
OTUabund <-cbind(flipped_seqtab.nochim, taxa)
write.csv(OTUabund, file ="/Users/anxinzhao/Desktop/assignment3/OTUabund.csv" )
```

```{r}
#construct a simple sample data.frame from the information encoded in the filenames (usually this step would instead involve reading the sample data in from a file).
samples.out <- rownames(seqtab.nochim)
samdf <- data.frame(samples.out)
rownames(samdf) <- samples.out
```

```{r}
#load libraries
library(phyloseq); packageVersion("phyloseq")
library(Biostrings); packageVersion("Biostrings")
library(ggplot2); packageVersion("ggplot2")
```
```{r}
#convert objects into matrix
seqtab.nochim<-as.matrix(seqtab.nochim)
taxa<-as.matrix(taxa)
```

```{r}
#make phyloseq object
ps <-phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
```


```{r}
#store the DNA sequences of our ASVs in the refseq slot of the phyloseq object, and rename taxa to a short string
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```
```{r}
#relative abundance
ps.prop <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
#melt phyloseq data object into dataframe
ps.proptable <- psmelt(ps.prop)
#factor phylum
ps.proptable$Phylum<-factor(ps.proptable$Phylum)
```

```{r}
#original plot
plot_bar(ps, fill="Phylum")
p <- plot_bar(ps, fill="Phylum")
p+geom_bar (aes(fill=Phylum), stat="identity", position="stack")+labs(title = "Relative Abundance", x = "sample", y = "relative abundance(%)",labels = c("Hummock","Neg","Trough"))
```
```{r}
#plot the graph of phyla
ggplot(data = ps.proptable, mapping=aes(x=Sample,y=Abundance))+geom_bar(aes(fill=Phylum), stat="identity", position="stack")+scale_x_discrete(limit = c("Nig-ALH1","Nig-ALNeg","Nig-ALT2"),labels = c("Hummock","Neg","Trough"))+labs(y="relative abundance",title="Relative Abundance of Phyla")

```


```{r}
#plot the graph of order
ggplot(data = ps.proptable, mapping=aes(x=Sample,y=Abundance))+geom_bar(aes(fill=Order), stat="identity", position="stack")+scale_x_discrete(limit = c("Nig-ALH1","Nig-ALNeg","Nig-ALT2"),labels = c("Hummock","Neg","Trough"))+labs(y="relative abundance",title="Relative Abundance of Order")

```
```{r}
#find the top 3 shared ASVs
#subset a smaller dataset
phylo2 = subset_samples(ps, samples.out == "Nig-ALH1" | samples.out == "Nig-ALNeg")
#remove all OTUs not found at least once in both samples
phylo2 = filter_taxa(phylo2, function(x) sum(x >= 1) == (2), TRUE)
#display the OTU table
otu_table(phylo2)
```
```{r}
#find the top 3 shared ASVs
#subset a smaller dataset
phylo3 = subset_samples(ps, samples.out == "Nig-ALNeg" | samples.out == "Nig-ALT2")
#remove all OTUs not found at least once in both samples
phylo3 = filter_taxa(phylo3, function(x) sum(x >= 1) == (2), TRUE)
#display the OTU table
otu_table(phylo3)
```



```{r}
#make a geom_point graph
ggplot(data = ps.proptable, mapping=aes(x=Sample,y=Phylum,))+geom_point(aes(color=Phylum,size =Abundance),position="nudge")+scale_x_discrete(limit = c("Nig-ALH1","Nig-ALNeg","Nig-ALT2"),labels = c("Hummock","Negative control","Trough"))+labs(y="relative abundance",title="Relative Abundance of Phyla")
```
